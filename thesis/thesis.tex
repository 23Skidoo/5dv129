\documentclass[a4paper,11pt,oneside]{report}

\usepackage[noindent]{UmUThesis}  % Non indented English

\usepackage[utf8]{inputenc}
\usepackage[urw-garamond]{mathdesign}
\usepackage[defaultmono, scale=0.8]{droidmono}
\usepackage[bookmarksnumbered, unicode, pdftex, hidelinks]{hyperref}
\usepackage[kerning,spacing]{microtype}
\usepackage{tocloft}

\renewcommand{\cftpartleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}

\title{A Cross-Platform Scalable I/O Manager for GHC}
\subtitle{Improving Haskell Networking on Windows}
\author{Mikhail Glushenkov}
\supervisor{Jerry Eriksson}
\examiner{Suna Bensch}
\semester{Spring 2016}
\course{Bachelor's thesis, 15 credits}
%\education{}

\graphicspath{{pictures/}}

\pagestyle{empty}

\begin{document}
\maketitle

\chapter*{Abstract}

% 10-20 lines
Haskell is a popular functional programming language. GHC is an
industrial-strength implementation of Haskell that has a number of features
making it a very attractive platform for writing high-performance network
applications. Unfortunately, support for modern scalable network I/O APIs in the
GHC runtime system is currently limited to Unix-like platforms. Haskell
applications targeting Windows therefore cannot attain the same levels of
scalability and performance as their Unix counterparts, and also have some
correctness problems.

A redesign of GHC's Windows I/O subsystem is proposed, making use of the Windows
I/O completion ports API. A proof of concept implementation of the design is
evaluated and found to be a strict improvement over the current state of
affairs. The improvements are transparently available to existing Haskell
applications.

\pagebreak

\chapter*{Acknowledgements}

This thesis builds upon the work performed by many different people over the
years. Johan Tibell and Bryan O'Sullivan implemented the initial version of the
GHC I/O manager for Unix. Andreas Voellmy and coauthors optimised and improved
the design of the I/O manager in many respects. Felix Martini wrote the
\texttt{winio} package for working with Windows asynchronous I/O in
Haskell. Joseph Adams is the author of the first version of Windows I/O manager.

I'd also like to express my gratitude to my thesis advisor, Jerry Eriksson, for
providing advice and guidance during my work on this thesis.

Everyone else, you know who you are.

\pagebreak


\tableofcontents

\pagebreak

\pagestyle{fancy}
\setcounter{page}{1}
\setcounter{chapter}{-1}

\chapter{Introduction}

Threads are a simple and powerful abstraction for implementing network
applications such as web servers. Using one thread per connection lets the
programmer express client-server communication in a modular way, simplifies
exception handling and resource management, and prevents requests that are
CPU-intensive to process from causing starvation by taking control over
scheduling out of programmer's hands. However, OS-level threads incur too much
of an overhead in terms of context-switching and fixed memory costs to make them
practical for high-performance applications; therefore, all modern APIs for
scalable network I/O are event-based. In the event-based model communication
with all clients is handled by just a single thread processing all incoming I/O
events in an infinite loop.

\textit{Haskell threads}, as implemented in GHC, combine the simplicity of the
thread-based model with the performance of the event-based approach. Haskell
threads have low memory and context-switching overhead, thus allowing to create
a large number of them (100K-1M), enough to associate a single thread with each
connection. They are multiplexed onto a much smaller number of OS threads,
usually set equal to the number of CPU cores. A separate I/O manager thread,
implemented as part of the runtime system, handles all I/O under the hood,
providing the Haskell programmer with a convenient synchronous API. In the
recent releases of GHC the I/O manager has been modified to use modern
event-based I/O APIs and heavily optimised, making it competitive with current
state of the art.

Sadly, those improvements are available only to GHC users on Unix-like platforms
due to differences between I/O event models on Windows (I/O completion ports)
and Unix (\texttt{epoll}/\texttt{kqueue}/\texttt{poll}). This thesis proposes a
way to solve this problem by redesigning GHC's Windows I/O subsystem to make use
of the I/O completion ports API. The evaluation of this redesign shows it to be
a strict improvement over the current state of affairs.

The rest of this document is organised as follows: Chapter~\ref{chap:background}
provides an overview of the background necessary for understanding the rest of
the thesis; Chapter~\ref{chap:old-io-manager} outlines the previous attempt at
implementing a Windows I/O manager for GHC; Chapter~\ref{chap:new-io-manager}
contains a description of the proposed new design for the Windows I/O manager;
Chapter~\ref{chap:evaluation} presents an empirical evaluation of a proof of
concept implementation of the new design; Chapter~\ref{chap:related-work}
discusses related work; and finally, Chapter~\ref{chap:conclusions} contains
some final words and reflections about possible future directions.

\chapter{Background}
\label{chap:background}

This chapter provides some necessary background required for understanding the
remainder of the thesis. It gives a short overview of Haskell, Glasgow Haskell
Compiler and the Concurrent Haskell extension, and describes GHC's input/output
subsystem.

\section{Haskell}

Haskell~\cite{bib:haskell2010} is a popular programming language with a unique
combination of features making it highly attractive for programming
high-performance networking applications. Specifically, Haskell has a powerful
static type system, focus on immutability, and advanced support for concurrent
and parallel programming~\cite{bib:marlow2013}. GHC, the flagship Haskell
compiler, generates fast native code and has a highly optimised network I/O
layer~\cite{bib:voellmy}. One unique aspect of Haskell is the non-strict
(call-by-value) evaluation strategy.

The origins of Haskell can be traced to a 1987 meeting at a FPCA conference in
Portland, Oregon, when a number of programming language researchers decided to
agree on a standard for a lazy functional programming language to serve as a
common foundation for application development and a research laboratory for
testing new ideas~\cite{bib:hudak2007}. Since then, Haskell continued to evolve
and grow in popularity, gaining a measure of industry adoption and a number of
advanced language extensions.

Experimental type system features notwithstanding, the attractiveness of Haskell
to a working programmer is largely due to three factors: high level of
abstraction, statically enforced type safety, and ease of refactoring.

Haskell code is high-level and concise. Its pure nature (by default functions in
Haskell cannot have side effects or mutate data) makes it easier to reason about
code, since a pure function will always have the same result for a given set of
inputs. Its concise syntax makes the cost of creating helper (often local)
functions low, and higher-order functions make such helper functions more
powerful. This makes idiomatically written Haskell code less repetitive and
encourages expressing the problem domain in a more abstract way.

Haskell is type safe. According to Robin Milner's well-known quote, ``well-typed
programs cannot go wrong''. While originally that referred only to memory
safety, a lot of Haskell-related research has gone into making Haskell programs
not ``go wrong'' in more interesting ways. For example, functions with pure
types are statically guaranteed to not have any side effects; for more advanced
examples, see~\cite{bib:gibbons2003}. Type-correct Haskell programs are also
widely perceived to not be wrong to begin with: when the code compiles, it often
just works~\footnote{See \url{https://wiki.haskell.org/Why_Haskell_just_works}
  for more on this topic.}. Type inference removes the syntactic burden of
manually written type annotations and, again, makes the code more concise.

Ease of refactoring is probably the killer feature of Haskell. Static type
checking makes refactoring safe and easy, and therefore large refactorings are
much more common in Haskell than in many other languages. This ability means
that it's easier for the code to evolve with changing requirements, thus making
maintenance and continued development cheaper.

All this made Haskell one of the most popular languages in its niche (statically
typed, garbage collected, functional). Hackage, the online repository of Haskell
libraries, holds around ten thousand different packages, and every day there are
about 1500 people communicating on the \texttt{\#haskell} chat channel on
\texttt{\#freenode}.

\section{GHC}

GHC~\cite{bib:ghc} is a flagship industrial-strength implementation of Haskell
developed initially at the University of Glasgow (hence the name, Glasgow
Haskell Compiler), and later at Microsoft Research Cambridge and by a large
number of contributors worldwide. GHC is by now synonymous with Haskell, and
while there is an official Haskell language standard~\cite{bib:haskell2010}, it
is considered a bit conservative, and therefore a serious production code base
written in Haskell will most likely utilise a number of GHC-specific extensions.

GHC is open source. It is largely written in Haskell itself, though some parts
relevant for the topic of this work are in C. It compiles to fast native code --
side-effect free nature of idiomatic Haskell makes it possible to implement
advanced code transformation passes not possible in other languages. However,
the mismatch between the Haskell execution model and the machine model
translates into Haskell code usually being somewhat slower than C without manual
optimisation.

A large number of target architectures is supported, though only x86 and x86-64
are currently Tier 1. Linux, Windows, OS X, and FreeBSD are Tier 1 supported
target operating systems. An extensive runtime system (RTS for short) provides a
parallel, generational garbage collector, a load-balancing multicore scheduler
for Haskell threads, and support for profiling, exception handling and other
tasks associated with running compiled Haskell code.

Besides having an abundance of type system and language extensions and
state-of-the-art code optimisation capabilities, GHC is also renowned for its
stellar support for concurrency and parallelism. While a number of different
models and abstractions for doing parallel and concurrent programming are
supported~\cite{bib:marlow2013}, the Concurrent Haskell extension is the most
relevant one for the purposes of this thesis.

\section{Concurrent Haskell}

The Concurrent Haskell programming model revolves around the concept of
user-level or ``green'' threads: lightweight threads that are multiplexed onto a
much smaller number of operating system (OS) threads, also called Capabilities
or Haskell Execution Contexts (HECs) in GHC parlance. The number of HECs is
usually set to be equal to the number of CPU cores. The lightweight character
makes it possible to create a large number (100K-1M) of Haskell threads; this
can be used, for example, to associate a thread with each open connection in a
network server.

The following program listing provides an example of a simple server implemented
in Concurrent Haskell:

\begin{verbatim}
    main :: IO ()
    main = do
        listenSock <- socket AF_INET Stream defaultProtocol
        bind listenSock (SockAddrInet portNumber iNADDR_ANY)
        forever $ do
            commSock <- accept listenSock
            forkIO $ worker commSock

    worker :: Socket -> IO ()
    worker commSock = do
       inp <- recvAll commSock
       sendAll commSock inp

\end{verbatim}

Here, the \texttt{main} IO action creates a socket and binds it to some port. It
then starts an infinite loop in which it listens for new connections on that
socket, and for each new connection forks off a new lightweight worker
thread. The \texttt{worker} IO action handles all client communication; in this
example it just echoes all input back to the client. An important detail is that
all input/output operations are \textit{synchronous}: \texttt{recvAll} and
\texttt{sendAll} block until all data has been received and sent, respectively.

In addition to the ability to create new threads, Concurrent Haskell provides a
number of facilities for inter-thread synchronisation and communication. For the
purposes of this work it suffices to only look at MVars -- a basic
synchronisation primitive on top of which more sophisticated structures such as
channels and semaphores can be built. An MVar is essentially a mutable location
protected by a mutex; a minimal useful interface for working with MVars looks
like the following:

\begin{verbatim}
    data MVar a

    newEmptyMVar :: IO (MVar a)
    putMVar      :: MVar a -> a -> IO ()
    takeMVar     :: MVar a -> IO a
\end{verbatim}

An MVar can be either empty of full. The \texttt{newEmptyVar} operation creates
a new empty MVar statically constrained to hold values of some type
\textit{a}. The \texttt{putMVar} operation puts some value of type \textit{a}
into an empty MVar. Trying to put a value into a non-empty MVar generates an
exception. The \texttt{takeMVar} operation returns the contents of a given MVar,
making it empty. If the MVar is empty to start with, \texttt{takeMVar} blocks
until it's full; if there're multiple threads waiting on a single MVar, only one
thread is woken up and the remaining ones are served in a fair (FIFO) order.

\section{Foreign Calls}
\label{sec:foreign-calls}

In the example in the previous section it was shown that an input/output
operation can block a Haskell thread. It is clear, however, that an operation
that blocks a Haskell thread must not be allowed to block the OS thread that
it's executing on: otherwise the remaining Haskell threads will never get a
chance to run.

Safe foreign calls are a mechanism for preventing blocking or long-running calls
to foreign (usually C) library code issued by a Haskell thread from blocking the
underlying OS thread. A safe foreign call gets compiled to the following native
code sequence (using C syntax):

\begin{verbatim}
    suspendThread();
    safeForeignCall();
    resumeThread();
\end{verbatim}

The \texttt{suspendThread()} and \texttt{resumeThread()} operations are both
primitives that are part of the GHC runtime system. The \texttt{suspendThread()}
call removes the current Haskell thread from the HEC's run queue, releases the
lock associated with the current HEC, wakes up an OS thread from a worker pool
associated with the current HEC (or creates a new one in case the worker pool is
empty), and hands off the HEC to that worker thread, which now proceeds with
executing Haskell code. The actual foreign call is performed in the next step,
after which the \texttt{resumeThread()} operation puts the current Haskell
thread back on the HEC's run queue and adds the current worker thread to the
HEC's worker pool and goes to sleep, or just shuts the worker thread down when
the HEC's worker pool is full.

One downside of safe foreign calls is that they can't be interrupted by
asynchronous exceptions (such as timeouts). This can be mitigated by using an
\textit{interruptible} foreign call (enabled by the \texttt{InterruptibleFFI}
extension). The runtime system will try to interrupt the thread blocked in such
a call with an OS-level mechanism (a \texttt{SIGPIPE} signal on Unix systems or
\texttt{CancelSynchronousIO} on Windows); otherwise they behave like safe ones.

Finally, there are unsafe foreign calls. An unsafe foreign call gets compiled to
just a simple \texttt{call} instruction, therefore avoiding the runtime
bookkeeping overhead described above. The downside is that unsafe foreign calls
are both blocking and uninterruptible. Unsafe foreign calls can be useful in
highly optimised low-level code, but should be handled with care.

\section{GHC I/O Manager}

The safe foreign call mechanism, while sufficient for relatively infrequent and
long-running foreign calls, is not performant enough for highly concurrent
network applications. As was shown in the previous section, using one Haskell
thread per connection means that a new OS thread has to be allocated for each
\texttt{recv} or \texttt{send} foreign call. This is problematic because a high
number of native threads incurs unacceptable context-switching and memory
overhead (each native thread requires approximately 1 MiB of memory for the
native stack and associated structures).

This problem has been long recognised by the designers of modern operating
systems. Modern APIs for doing scalable I/O, such as \texttt{kevent} and
\texttt{epoll}, use an \textit{event-based} programming model, in which a single
OS thread can handle multiple concurrent connections. Let's look at the
interface provided by the Linux \texttt{epoll} mechanism to see what this means
in practice:

\begin{verbatim}
    int epoll_create(int size);

    int epoll_ctl(int epfd, int op, int fd,
                  struct epoll_event *event);

    int epoll_wait(int epfd,
                   struct epoll_event *events,
                   int maxevents, int timeout);
\end{verbatim}

The \texttt{epoll\_create} call creates a new \texttt{epoll} object. The client
can then associate a number of file descriptors with that object by using the
\texttt{epoll\_ctl} call. For each file descriptor, the client can choose which
types of events are watched by the \texttt{epoll} object -- for example, the
client can choose to wait for the file descriptor to become available for
reading, writing, or both. Finally, the \texttt{epoll\_wait} call blocks until
any of the events registered with \texttt{epoll\_ctl} actually happen, and then
allows to find out which ones did, and for which file descriptors.

A typical network server using this API will use just a single OS thread for
processing all incoming events. During the setup phase the application will
create a socket for listening on some port, register it with the \texttt{epoll}
object, and enter an infinite \textit{event loop}, in which it will alternate
between waiting for new events with \texttt{epoll\_wait} and processing
them. Each new connection will trigger an event on the socket listening on the
server port; this will cause the application to register a fresh socket
associated with that connection with the \texttt{epoll} object. Once a
connection socket becomes ready for reading or writing, it will in turn trigger
new events that the application will process; and once the application is done
with a connection, it will close the connection socket and deregister it from
the \texttt{epoll} object.

The main advantage of this model is its improved scalability and performance
compared with using one OS thread per connection; the main disadvantage is that
it is much less convenient to program with. Using one thread per connection lets
the programmer express her intent in a modular way, simplifies exception
handling and resource management (all resources dedicated to a single connection
can be freed when the associated thread exits), and makes it easier to prevent
CPU-intensive code processing a single request from starving other request
handlers of CPU time (since control over preemption is taken out of the
programmer's hands).

Happily, modern versions of GHC allow to combine the performance of the
event-based model with the convenience of the thread-based one. This is achieved
by the runtime system component called the \textit{I/O
  manager}~\cite{bib:o'sullivan}. Simply put, the I/O manager is just another
Haskell thread, implemented as part of the runtime system, that runs an
\texttt{epoll}- or \texttt{kevent}-based event loop and notifies all other
Haskell threads when new I/O events become available. Since the I/O manager
handles all I/O for all Haskell threads, it can afford to use the safe foreign
call mechanism for calls to \texttt{epoll\_wait} and related system functions;
since the I/O manager is a part of the runtime and only exits when the whole
program shuts down, it doesn't have to worry about asynchronous exceptions
either\footnote{In fact, as will be shown later, the I/O manager is actually
  used for \textit{implementing} timeouts.}. The I/O manager exposes the
following semi-public API to Haskell code:

\begin{verbatim}
    threadWaitRead  :: Fd -> IO ()
    threadWaitWrite :: Fd -> IO ()
\end{verbatim}

A \texttt{threadWaitWrite} call creates an empty MVar, registers the provided
file descriptor with the I/O manager, and blocks on that MVar. Once that file
descriptor becomes ready for writing, the I/O manager runs the callback provided
by \texttt{threadWaitWrite}, which fills in that MVar, waking the Haskell thread
blocked in \texttt{threadWaitWrite}. The Haskell thread can then proceed with
writing to that file descriptor. The \texttt{threadWaitRead} call works in the
same way, the only difference is that blocks until a file descriptor becomes
ready for reading instead of writing.

Libraries for doing network I/O in Haskell are written to cooperate with this
API, exposing the familiar synchronous \texttt{send}/\texttt{recv}-style
interface to the programmer. Thus the context-switching and memory overhead
associated with using one OS thread per connection is avoided in the GHC
implementation of Concurrent Haskell. In recent GHC versions the I/O manager has
been further optimised~\cite{bib:voellmy}, and its performance is currently on
par with the state of the art in the area.

\section{Bound threads}
\label{sec:bound-threads}

In addition to normal Haskell threads, GHC runtime also supports a variant
called bound threads. Those are Haskell threads that have an associated OS
thread and are guaranteed to only ever be executed in the context of that OS
thread. This is useful for communicating with foreign libraries that are
sensitive to the ID of the calling OS thread -- for example, libraries that use
thread-local-storage internally. Otherwise, bound threads are just like normal
Haskell threads, the only difference is that they are created with the
\texttt{forkOS} operation instead of \texttt{forkIO}.

\chapter{Old Windows I/O Manager}
\label{chap:old-io-manager}

GHC on Windows currently lacks an I/O manager~\cite{bib:ticket7353}. All network
I/O is performed using the safe foreign call mechanism described in the previous
chapter. Besides the performance and scalability problems outlined above, this
means that I/O operations such as \texttt{send} and \texttt{recv} cannot be
interrupted by asynchronous exceptions (e.g. a timeout or a keyboard interrupt).

However, there does exist an abandoned set of patches\footnote{Original author:
  Joseph Adams.} implementing a Windows I/O manager for
GHC~\cite{bib:ticket7353}. Work described in this thesis builds on that early
implementation. This chapter outlines the approach taken during that previous
attempt and the problems that prevented it from being merged into mainline GHC.

\section{I/O completion ports}

The main reason that a Windows I/O manager hasn't yet been added to GHC (besides
the lack of resources) is that the approach chosen by the designers of the
Windows API for doing scalable event-based I/O is quite different from the one
used by Unix-like systems (Linux, FreeBSD, OS X/Darwin). As was shown in the
previous chapter, on Unix a program utilising \texttt{epoll} or \texttt{kevent}
registers a number of file handles with the OS and waits for them to become
available for reading or writing. However, with the I/O completion ports (or
IOCP for short), as the Windows API is called, it is the other way around: the
program starts all its I/O operations \textit{asynchronously}, and waits for
them to complete.

Let's look at the IOCP API in more detail. For the purposes of this discussion,
it can be boiled down to the following five functions:

\begin{verbatim}
    HANDLE CreateIoCompletionPort(
      HANDLE    FileHandle,
      HANDLE    ExistingCompletionPort,
      ULONG_PTR CompletionKey,
      DWORD     NumberOfConcurrentThreads
    );

    int WSASend(
      SOCKET                             s,
      LPWSABUF                           lpBuffers,
      DWORD                              dwBufferCount,
      LPDWORD                            lpNumberOfBytesSent,
      DWORD                              dwFlags,
      LPWSAOVERLAPPED                    lpOverlapped,
      LPWSAOVERLAPPED_COMPLETION_ROUTINE lpCompletionRoutine
    );

    BOOL GetQueuedCompletionStatus(
      HANDLE       CompletionPort,
      LPDWORD      lpNumberOfBytes,
      PULONG_PTR   lpCompletionKey,
      LPOVERLAPPED *lpOverlapped,
      DWORD        dwMilliseconds
    );

    BOOL WINAPI CancelIo(
      HANDLE hFile
    );
\end{verbatim}

An I/O completion port is an abstract OS-level queue object similar to the
\texttt{epoll} object created by \texttt{epoll\_create}. The IOCP API function
\texttt{CreateIoCompletionPort} serves the same purpose as
\texttt{epoll\_create} and \texttt{epoll\_ctl} do for \texttt{epoll}. It is used
for both creating new I/O completion ports and associating existing ones with
file handles.

Asynchronous I/O operations are initiated by standard system functions like
\texttt{ReadFile} and \texttt{WriteFile}. For this example the \texttt{WSASend}
function from the Windows Sockets API was chosen because of this thesis's focus
on network I/O. The arguments \texttt{WSASend} accepts are almost the same ones
that the \texttt{send} system call accepts on Unix: a socket handle, a buffer
pointer, the number of bytes in the buffer, an output argument for recording the
number of bytes actually sent, and a flag word. By default \texttt{WSASend}
sends some data over a socket, blocking until it is done.

The \texttt{lpCompletionRoutine} argument can be ignored for the purposes of
this discussion, leaving only the \texttt{lpOverlapped} argument, which is a
pointer to an \texttt{OVERLAPPED} structure. The internals of the
\texttt{OVERLAPPED} structure can be, again, ignored for the purposes of this
discussion. The important thing is that when this pointer is non-zero and the
socket has been marked as overlapped during creation, the send operation is
asynchronous.

The \texttt{GetQueuedCompletionStatus} function is IOCP's analogue of
\texttt{epoll\_wait}. It blocks until some I/O operation initiated on one of the
file handles associated with a given I/O completion port actually completes
and returns some data about the completed operation. The \texttt{dwMilliseconds}
argument allows to perform a non-blocking poll or block for a limited amount of
time.

Finally, the \texttt{CancelIo} function cancels all asynchronous I/O operations
on the given file handle initiated by the calling OS thread.

Additional information on the I/O completion ports API is available in
Microsoft's official documentation online~\cite{bib:msdn}.

\section{Windows Event Loop}

As is the case with the GHC I/O manager on Unix, the heart of the unofficial
Windows GHC I/O manager is the event loop. In simplified form, a single
iteration of the event loop looks like this:

\begin{verbatim}
    step = do
        runExpiredTimeouts
        m <- getNextCompletion
        case m of
            Nothing                      -> return ()
            Just (cb, numBytes, errCode) -> cb errCode numBytes
\end{verbatim}

Let's ignore \texttt{runExpiredTimeouts} for a moment. As can be seen, the event
loop is quite simple and mainly involves repeated calls to a Haskell procedure
\texttt{getNextCompletion}, which is a wrapper over a safe foreign call to
\texttt{GetQueuedCompletionStatus}. When \texttt{getNextCompletion} returns, it
either means that some I/O operation has completed or that the time slice
specified via the \texttt{dwMilliseconds} argument of
\texttt{GetQueuedCompletionStatus} has been exceeded. In the former case
\texttt{getNextCompletion} returns a callback associated with the completed
operation, which is immediately executed.

In addition to taking care of input and output, the unofficial Windows I/O
manager also handles timeouts. This part is, again, similar to the Unix I/O
manager, which also has to deal with timeouts\footnote{Discussion of this topic
  was omitted from the previous chapter in the interests of clarity.}. Timeout
handling is, again, pretty simple: the internal I/O manager state structure
contains a priority search queue that records all timeouts currently registered
in the system. The \texttt{runExpiredTimeouts} operation removes all timeouts
that expired during the latest time slice from the queue and executes the
callbacks associated with those expired timeouts.

Just like the standard Unix I/O manager, the unofficial Windows I/O manager
provides a semi-public interface for implementing synchronous user-level
wrappers on top of low-level asynchronous I/O operations. As was explained in
the previous section, the \texttt{threadWaitRead} / \texttt{threadWaitWrite}
operations can't be efficiently implemented on Windows. Therefore, this
interface looks a bit differently:

\begin{verbatim}
    associateHandle :: Manager -> HANDLE -> IO ()

    withOverlapped :: Manager
                   -> HANDLE
                   -> StartCallback
                   -> CompletionCallback a
                   -> IO a

    type StartCallback = Overlapped -> IO ()

    type CompletionCallback a = ErrCode
                             -> DWORD
                             -> IO a
\end{verbatim}

Here, the \texttt{Manager} type represents a reference to the internal I/O
manager state structure that can be obtained via a call to the
\texttt{getSystemManager} operation. The \texttt{associateHandle} operation
simply associates a file handle with the I/O manager's I/O completion port
(as explained in the previous section, that file handle must be properly set
up for doing asynchronous I/O).

The \texttt{withOverlapped} operation takes a file handle, a callback for
starting an asynchronous I/O operation, and a callback that should be called on
completion. It allocates a new \texttt{OVERLAPPED} structure, invokes the start
callback, creates a new empty MVar, registers the I/O operation with the I/O
manager and blocks on the MVar it has just created. When the I/O operation
completes, the I/O manager invokes the completion callback and signals the MVar,
causing \texttt{withOverlapped} to return control to the caller.

There are two further complications that the Windows I/O manager has to deal
with: canceling asynchronous I/O operations and sensitivity to the ID of the
calling thread. When a Haskell thread blocked in \texttt{withOverlapped}
receives an asynchronous exception, it must cancel the unfinished I/O operation
it has just initiated. As was shown in the previous section, the
\texttt{CancelIo} function simply cancels all asynchronous I/O operations on a
given file handle issued by the calling OS thread; this can be problematic
if two different Haskell threads have initiated I/O operations on the same file
handle from the same OS thread, but only one of the Haskell threads needs to
be canceled.

Similarly, asynchronous I/O operations in Windows are also sensitive to the ID
of the calling OS thread. When an OS thread that initiated an asynchronous I/O
operation exits, that operation is canceled. As was explained in
Section~\ref{sec:foreign-calls}, GHC runtime maintains a fixed-size pool of
native worker threads that are considered interchangeable for all intents and
purposes. An OS thread that has at some point initiated an asynchronous I/O
operation can be simply shut down by the runtime at any moment.

Therefore the unofficial Windows I/O manager enforces the following invariant:
each file handle registered with the I/O manager has an associated pool of
bound threads (see Section~\ref{sec:bound-threads}), and each bound thread in
the pool can have at most one pending I/O operation per file
handle. However, bound threads can be shared between pools associated with
different file handles. Calls to \texttt{CancelIo} are always issued from
the same bound thread (and therefore OS thread) that initiated the I/O
operation. A bound thread that is a part of a pool associated with some file
handle never exits until all pending I/O operations initiated from that
thread have completed.

\section{Shortcomings of the old I/O Manager}
\label{sec:shortcomings-old-io-manager}

The unofficial Windows I/O manager, as described above, is reasonably complete
and functional; it even solves the problem of network I/O operations being
uninterruptible by asynchronous exceptions. However, the overhead of maintaining
multiple pools of bound threads makes it unsuitable for practical use. A simple
benchmark measuring the performance effect of enabling the unofficial Windows
I/O manager by default (two Haskell threads exchanging messages over a socket)
showed an order of magnitude difference compared with the standard Haskell
network library (implemented on top of safe foreign calls on Windows).

Another problem of the unofficial Windows I/O manager is that one has to add
support for the \texttt{withOverlapped} interface to all standard Haskell I/O
libraries in order to actually take advantage of it. This, unfortunately, is a
fundamental limitation forced by using IOCP; but see
Chapter~\ref{chap:conclusions} for some discussion on this topic.


\chapter{New Windows I/O Manager}
\label{chap:new-io-manager}

As was shown in the previous chapter, the existing attempt at implementing a
Windows I/O manager for GHC suffers from performance problems making it unusable
in practice and unsuitable for inclusion in mainline GHC. This chapter describes
a new Windows I/O manager design that attempts to solve or at least alleviate
those problems. Most ideas discussed in this chapter were originally proposed in
the~\cite{bib:voellmy} paper. A partial proof of concept implementation of this
design is the focus of the next chapter.

\section{Removing bound thread pools}

As was explained in Section~\ref{sec:shortcomings-old-io-manager}, the main
problem with the old Windows I/O manager is the overhead of maintaining multiple
pools of bound threads and the associated context switching costs. Getting rid
of the bound thread pools would probably make the I/O manager code at least as
fast as the standard network library, if not faster.

The first reason for implementing the bound thread pool scheme was the limited
functionality of the \texttt{CancelIo} function: it cannot cancel specific I/O
operations, and it cannot cancel I/O operations initiated by other OS
threads. Fortunately, in Windows Vista Microsoft introduced the following
function that lifts both of those restrictions:

\begin{verbatim}
    BOOL CancelIoEx(
      HANDLE       hFile,
      LPOVERLAPPED lpOverlapped
    );
\end{verbatim}

This function is not sensitive to the ID of the calling thread, and the optional
\texttt{OVERLAPPED} argument that it accepts allows to identify a specific I/O
operation previously initiated by some thread in the current process. Not
supporting Windows versions earlier than Vista used to be a concern for some
time, but it appears that starting with the GHC 8 release pre-Vista versions of
Windows are no longer supported anyway~\footnote{See
  \url{https://ghc.haskell.org/trac/ghc/wiki/WindowsGhc}.}.

The second reason for having a bound thread pool scheme was that all
asynchronous I/O initiated by a given OS thread is canceled when that thread
exits. Given that the GHC runtime starts and shuts down native worker threads as
it pleases, this means that any operation initiated by a Haskell thread can in
principle be randomly canceled at any moment. Working around that restriction is
a bit more burdensome. What is needed is a way to prevent the GHC runtime from
prematurely terminating native worker threads that have pending I/O
operations. This can be implemented by adding a reference-counting system for
native worker threads to the runtime system with the following Haskell
interface:

\begin{verbatim}
    incrTaskRefCount :: IO (IO ())

    withIncrTaskRefCount :: IO a -> IO a
\end{verbatim}

The \texttt{incrTaskRefCount} operation increases the reference count for the OS
thread (or Task in GHC RTS parlance) that the current Haskell thread is running
on and returns an action that decreases the reference count for that thread when
run (this is needed because Haskell thread can migrate between Tasks). A Task
whose reference count is non-zero can never get forcibly shut down by the run
time system even if the number of Tasks in the HEC worker pool exceeds the
maximum threshold. To prevent the number of Tasks in the HEC worker pool from
growing uncontrollably the runtime system tries to shut down the excess Tasks
whose reference count is zero during the garbage collection phase.

The \texttt{withIncrTaskRefCount} operation is just an exception-safe Haskell
wrapper for \texttt{incrTaskRefCount}. It increases the reference count of the
current Task for the duration of the IO action given to it as an argument. It is
used in the implementation of \texttt{withOverlapped} for managing the reference
counts of Tasks with pending I/O operations:

\begin{verbatim}
    withOverlapped = withIncrTaskRefCount $ do
        [...]
\end{verbatim}

As discussed in the next chapter, these two optimisations, when implemented,
actually do make the Windows I/O manager competitive with the standard Haskell
network library.

\section{Scalable registrations}

The remaining features of the new Windows I/O manager design are aimed at
improving its performance and scalability beyond the current status quo on
Windows. The implementation status of these optimisations at the time of writing
is still work in progress. However, it is nevertheless instructive to briefly
discuss them here. These optimisations are based on the pioneering work in
the~\cite{bib:voellmy} paper.

The first bottleneck identified in the~\cite{bib:voellmy} paper is the callback
registration mechanism used by the I/O manager. In the original implementation
it was just a hash table protected by a mutex; when a large number of Haskell
threads tried to update that hash table at the same time that caused contention
and degraded performance. That problem was solved by an additional level of
indirection: using a small fixed number ($2^5$) of mutex-protected hash tables
instead of just one, plus a read-only hash table on top. To register a callback
in this scenario the client first maps the file handle it is interested in
to one of the $2^5$ mutable hash tables using the top-level read-only one, and
then modifies the appropriate mutable hash table. The I/O manager uses the same
method to find out which callback to invoke.

This technique should be directly applicable for the Windows I/O manager with a
modest modification to its internal state structures and the code of
\texttt{withOverlapped}.

\section{Multiple I/O manager threads}

As discussed in the~\cite{bib:voellmy} paper, a single I/O manager thread in a
highly concurrent server application simply cannot saturate all HECs with work
beyond 4 cores. This was resolved by parallelising the I/O manager and setting
the number of I/O manager threads equal to the number of HECs (which is usually
the same as the number of cores).

This technique should be also possible to apply in the case of the Windows I/O
manager. The \texttt{withOverlapped} function will have to be modified to
register the completion callback with the I/O manager thread associated with the
current HEC instead of the global one. This optimisation should improve core
utilisation (by distributing the I/O manager load more evenly across cores) and
locality (by minimising the need for cross-HEC communication).

\section{Avoiding the overhead of blocking calls}

Another optimisation discussed in the~\cite{bib:voellmy} paper is avoiding
expensive safe foreign calls in the event loop by using a couple of initial fast
non-blocking unsafe calls to poll for ready events before proceeding with the
slow blocking call. After each non-blocking poll the I/O manager thread
voluntarily yields the control to the RTS scheduler, causing the I/O manager
thread to be moved to the end of the current HEC's run queue. This speeds things
up in the case where a large number of events is being generated and processed
by avoiding an expensive context switch incurred by the safe foreign
call. Voluntary yielding of control, however, increases the latency of timeout
handling, so that component has to be split into its own Haskell thread.

This optimisation should be directly applicable for the Windows I/O manager as
well with minimal modifications to the event loop. The timer manager code from
the Unix version can probably be reused.

Finally, the~\cite{bib:voellmy} paper discusses a technique for working around
an internal synchronisation bottleneck in the Linux kernel. This optimisation
seems to be Linux-specific; however, it may provide insight into further
optimisation of the Windows I/O manager.

\chapter{Empirical Results}
\label{chap:evaluation}

A prrof of concept version of the design described in the previous chapter was
implemented and experimentally evaluated using a number of simple
benchmarks. This chapter discusses the results obtained during the experimental
evaluation. The new design is shown to be competitive with the status quo on
Windows, and thus potentially appropriate for inclusion in GHC at some point in
the future.

\section{Proof of concept implementation}

The proof of concept implementation includes\ldots

\textbf{TODO: write}

\section{Experimental setup}

The benchmarks were run on a Windows machine with a 3.4 GHz four-core processor
and 16 GiB of RAM. The same machine was used as both client and
server. ApacheBench was used to test the network I/O performance. The numbers
reported are an average over five runs for each value of the input
parameter. Appendix~\ref{appendix:source-code} contains instructions for
replicating the experiment.

\section{Benchmark 1: PongServer}

The PongServer benchmark implements a simple HTTP server that sends a response
with a constant five-byte body (``Pong!'') in answer to any HTTP request. The
\texttt{pong} version of the server is implemented using the new I/O manager
prototype, and the \texttt{pong-baseline} version is implemented using the
standard Haskell network library.

The following table shows the number of requests per second as reported by
ApacheBench as a function of the number of connections (ApacheBench's
\texttt{-c} option):

\begin{center}
\begin{tabular}{ | l | l | l | }
  \hline
  Connections & \texttt{pong} & \texttt{pong-baseline} \\
  \hline
  10 & 10738 & 10526 \\
  \hline
  100 & 10810 & 10191 \\
  \hline
  1000 & 8290 & 8602 \\
  \hline
  10000 & 4885 & 4726 \\
  \hline
\end{tabular}
\end{center}

As can be seen above, code using the new I/O manager is competitive with code
that uses the standard network library on this benchmark.

\section{Benchmark 2: FileServer}

\begin{center}
\begin{tabular}{ | l | l | l | }
  \hline
  Connections & \texttt{file} & \texttt{file-baseline} \\
  \hline
  10 & 10289 & 9275 \\
  \hline
  100 & 10094 & 9142 \\
  \hline
  1000 & 8510 & 8163 \\
  \hline
  10000 & 4347 & 2844 \\
  \hline
\end{tabular}
\end{center}


\chapter{Related Work}
\label{chap:related-work}

\textbf{TODO: A review of literature with pointers to related papers and similar
  work done for systems other than GHC (for example, Go). }


\chapter{Conclusions and Future Work}
\label{chap:conclusions}

A new design for the Windows I/O manager for GHC was presented. A proof of
concept implementation of that design was shown to be an improvement over the
status quo when it comes to correctness; it was also shown to be competitive
when it comes to performance.

\textbf{TODO: expand.}

\section{Future Work}

A number of issues still need to be resolved before the current implementation
can be declared production-ready.

First, support for the new Windows I/O manager must be integrated with the
standard network library so that existing Haskell applications could take
advantage of it. There are some bits and pieces missing from the GHC RTS's
low-level I/O device abstraction that need to be implemented first.

Second, the new Windows I/O manager code must be extended to also handle file
I/O in addition to network I/O. This will likely require extensive changes to
the GHC runtime support library to make it use native Windows API types and
functions instead of the CRT wrapper.

Additionally, performance of the current implementation could be
improved. Besides implementing the full design described in
Chapter~\ref{chap:new-io-manager}, there are also some low-hanging fruit like
using the more efficient \texttt{GetQueuedCompletionStatusEx} API instead of
\texttt{GetQueuedCompletionStatus} and getting rid of \texttt{StablePtr}s in
favour of \texttt{ForeignPtr}s. If the full design were to be implemented,
testing its performance would require a more advanced setup akin to the one
described in~\cite{bib:voellmy} to saturate the network.

Finally, it'd be nice to share more code among the Windows and Unix I/O manager
implementations.  It should be possible to come up with an API that combines
\texttt{threadWaitRead} / \texttt{threadWaitWrite} and \texttt{withOverlapped}
by implementing an IOCP-style interface on top of
\texttt{epoll}/\texttt{kqueue}. The timeout manager code can probably be reused
wholesale.

\addcontentsline{toc}{chapter}{\bibname}
\begin{thebibliography}{9}

\bibitem[Haskell 2010]{bib:haskell2010}
  \emph{Haskell 2010 Language Report}\\
  \newblock Simon Marlow et al.\\
  \newblock \url{https://www.haskell.org/onlinereport/haskell2010/}\\
  \newblock URL accessed \today.

\bibitem[Hudak 2007]{bib:hudak2007}
  \href{http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf}{\emph{A History of Haskell: being lazy with class}}\\
  \newblock Paul Hudak, John Hughes, Simon Peyton Jones, Philip Wadler\\
  \newblock The Third ACM SIGPLAN History of Programming Languages Conference
  (HOPL-III) San Diego, California, June 9-10, 2007.

\bibitem[Marlow 2013]{bib:marlow2013}
  \href{http://community.haskell.org/~simonmar/pcph/}{\emph{Parallel and Concurrent
    Programming in Haskell}}\\
  \newblock Simon Marlow\\
  \newblock O'Reilly 2013, ISBN: 978-1449335946

\bibitem[Gibbons 2003]{bib:gibbons2003}
  \href{https://www.cs.ox.ac.uk/publications/books/fop/}{\emph{The Fun of Programmin}}\\
  \newblock ed. by Jeremy Gibbons and Oege de Moor\\
  \newblock Palgrace MacMillan, 2003.

\bibitem[GHC]{bib:ghc} \emph{The Glasgow Haskell Compiler: a technical
    overview}\\
  \newblock SL Peyton Jones, CV Hall, K Hammond, WD Partain, and PL Wadler\\
  \newblock Proceedings of Joint Framework for Information Technology Technical
  Conference, Keele, pp. 249-257.\\
  \newblock March 1993.

\bibitem[O'Sullivan 2010]{bib:o'sullivan}
  \href{http://research.google.com/pubs/pub36841.html}{\emph{Scalable I/O Event Handling for GHC}}\\
  \newblock Bryan O'Sullivan \& Johan Tibell.\\
  \newblock Proceedings of the 2010 ACM SIGPLAN Haskell Symposium (Haskell'10).

\bibitem[Voellmy 2013]{bib:voellmy}
  \href{http://haskell.cs.yale.edu/wp-content/uploads/2013/08/hask035-voellmy.pdf}{\emph{Mio:
      A High-Performance Multicore IO
      Manager for GHC}}\\
  \newblock Andreas Voellmy, Junchang Wang, Paul Hudak, Kazuhiko Yamamoto.\\
  \newblock Proceedings of the 2013 ACM SIGPLAN symposium on Haskell (Haskell'13).

\bibitem[Marlow 2004]{bib:marlow2004}
  \href{http://community.haskell.org/~simonmar/bib/concffi04_abstract.html}{\emph{Extending the Haskell Foreign Function Interface with Concurrency}}\\
  \newblock Simon Marlow, Simon Peyton Jones, Wolfgang Thaller\\
  \newblock Proceedings of the ACM SIGPLAN workshop on Haskell, pages 57--68,
  Snowbird, Utah, USA, September 2004

\bibitem[Marlow 2009]{bib:marlow2009}
  \href{http://community.haskell.org/~simonmar/bib/multicore-ghc-09_abstract.html}{\emph{Runtime Support for Multicore Haskell}}\\
  \newblock Simon Marlow, Simon Peyton Jones, Satnam Singh\\
  \newblock \emph{ICFP '09:} Proceeding of the 14th ACM SIGPLAN International
  Conference on Functional Programming, Edinburgh, Scotland, August 2009

\bibitem[GHC Trac]{bib:ticket7353} \emph{GHC Trac issue \#7353}\\
  \newblock \url{https://ghc.haskell.org/trac/ghc/ticket/7353}\\
  \newblock URL accessed \today.

\bibitem[MSDN]{bib:msdn} \emph{I/O Completion Ports}\\
  \newblock Microsoft Developer Network.\\
  \newblock
  \url{https://msdn.microsoft.com/en-us/library/windows/desktop/aa365198\%28v=vs.85\%29.aspx}\\
  \newblock URL accessed \today.

\end{thebibliography}

\appendix

\chapter{Accessing the source code}
\label{appendix:source-code}

Source code for the proof of concept implementation described in this thesis is
available on GitHub at \url{https://github.com/23Skidoo/haskell-iocp/}. Building
the code requires a patched version of GHC, which is available at
\url{https://github.com/23Skidoo/ghc}.

Follow these steps to build the source code:

\begin{itemize}
\item Clone the Git repository with patched GHC from
  \url{https://github.com/23Skidoo/ghc}.
\item Switch to the \texttt{ghc-8.0-max-spare-workers} branch.
\item Follow the instructions on GHC
  Wiki\footnote{\url{https://ghc.haskell.org/trac/ghc/wiki/Building}} to build
  GHC from source.
\item Make sure that you have \texttt{cabal-install} installed. A Windows binary
  can be downloaded from \url{https://www.haskell.org/cabal/download.html}. Make
  sure that the \texttt{cabal} tool is in \texttt{PATH}.
\item Clone the \texttt{haskell-iocp} Git repository from
  \url{https://github.com/23Skidoo/ghc}.
\item Run \texttt{cabal sandbox init}.
\item Run \texttt{cabal install -w c:/path/to/patched/ghc/inplace/bin/ghc-stage2.exe
    -{}-only-dependencies}.
\item Run \texttt{cabal configure -w c:/path/to/patched/ghc/inplace/bin/ghc-stage2.exe}.
\item Run \texttt{cabal build}.
\end{itemize}

This will build the I/O manager library and a number of benchmarks and test
programs. To run the test programs, use \texttt{cabal run}. For example,
\texttt{cabal run pong} starts the Pong HTTP server. Use ApacheBench or
\texttt{httpperf} for performance tests. Example invocation of ApacheBench:
\texttt{ab -n 10000 -c 100 http://127.0.0.1:8080/}. This makes ApacheBench send
ten thousand requests using one hundred simultaneous connections.

Windows x64 binaries for the \texttt{pong} and \texttt{file} benchmarks are also
available from \url{https://github.com/23Skidoo/haskell-iocp/releases/tag/0.1}.

\end{document}
